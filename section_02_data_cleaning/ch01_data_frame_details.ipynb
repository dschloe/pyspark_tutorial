{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfda780b-f3d4-4b44-bf50-36aa6a282439",
   "metadata": {},
   "source": [
    "# Spark Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ba9dfb-0130-4e51-ac81-acb3fd6aa3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "pplSchema = StructType([\n",
    "    # 이름 필드 정의\n",
    "    StructField('name', StringType(), True), \n",
    "    \n",
    "    # 나이 필드 정의\n",
    "    StructField('name', IntegerType(), True), \n",
    "    \n",
    "    # 도시 필드 정의\n",
    "    StructField('city', StringType(), True), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a42ec8-500f-457e-bacc-7e26e09a2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000230A36515E0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"MLSampleTutorial\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318cef6b-c4c9-4056-b52a-318b73019b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2014|         0005|                          519|    hnl|\n",
      "|       01/01/2014|         0007|                          505|    ogg|\n",
      "|       01/01/2014|         0035|                          174|    slc|\n",
      "|       01/01/2014|         0043|                          153|    dtw|\n",
      "|       01/01/2014|         0052|                          137|    pit|\n",
      "|       01/01/2014|         0058|                          174|    san|\n",
      "|       01/01/2014|         0060|                          155|    mia|\n",
      "|       01/01/2014|         0064|                          185|    jfk|\n",
      "|       01/01/2014|         0090|                          126|    ord|\n",
      "|       01/01/2014|         0096|                           91|    stl|\n",
      "|       01/01/2014|         0099|                          182|    sna|\n",
      "|       01/01/2014|         0103|                          181|    ont|\n",
      "|       01/01/2014|         0109|                          127|    den|\n",
      "|       01/01/2014|         0122|                          222|    sfo|\n",
      "|       01/01/2014|         0123|                          510|    hnl|\n",
      "|       01/01/2014|         0129|                          114|    cos|\n",
      "|       01/01/2014|         0130|                          141|    dca|\n",
      "|       01/01/2014|         0131|                          167|    slc|\n",
      "|       01/01/2014|         0132|                           82|    stl|\n",
      "|       01/01/2014|         0140|                          146|    bwi|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('data/AA_DFW_2014_Departures_Short.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0a0df-c8a0-4cb2-afa3-d1b7cefdee41",
   "metadata": {},
   "source": [
    "- parquet으로 저장함. \n",
    "- apache pyarrow install\n",
    "`pip install pyarrow'\n",
    "- 가상의 parquet 파일을 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d765964-fc9f-4740-bd13-4dd8bb1bfd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     one  two  three\n",
      "a   -1.0  foo   True\n",
      "b  100.0  bar  False\n",
      "c    2.5  baz   True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "one: double\n",
       "two: string\n",
       "three: bool\n",
       "__index_level_0__: string\n",
       "----\n",
       "one: [[-1,100,2.5]]\n",
       "two: [[\"foo\",\"bar\",\"baz\"]]\n",
       "three: [[true,false,true]]\n",
       "__index_level_0__: [[\"a\",\"b\",\"c\"]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "df = pd.DataFrame({'one': [-1, 100, 2.5], \n",
    "                   'two': ['foo', 'bar', 'baz'],\n",
    "                   'three': [True, False, True]}, index=list('abc'))\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'data/example.parquet')\n",
    "\n",
    "print(df)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad56920-9356-44a4-af55-f7ee3f8db73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/GREEN/Desktop/pyspark_tutorial/section_02_data_cleaning/spark-warehouse')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"MLSampleTutorial\").getOrCreate()\n",
    "spark.catalog.listTables('default')\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6843502-b2fb-4ea3-8f2f-3933af551009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988c300c-42b0-4805-a0c1-25821e88de25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a346599-08ce-4994-ada2-2e3af8fb9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-----------------+\n",
      "|  one|two|three|__index_level_0__|\n",
      "+-----+---+-----+-----------------+\n",
      "| -1.0|foo| true|                a|\n",
      "|100.0|bar|false|                b|\n",
      "|  2.5|baz| true|                c|\n",
      "+-----+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('data/example.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719e5f80-cf0a-4070-b5d7-405d4d027c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average value is: 33\n"
     ]
    }
   ],
   "source": [
    "example_df = spark.read.parquet('data/example.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "example_df.createOrReplaceTempView('example')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_val= spark.sql('SELECT avg(one) from example').collect()[0]\n",
    "print('The average value is: %d' % avg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157eb14-deb6-43c5-94a7-8ae8ea4a5276",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28df5e5-90ee-46f6-a224-4f9aff2379ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000230A36515E0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"dataCleansing\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a25759f-d876-42a9-9a92-aa5e81ecf1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = spark.read.format('csv').options(Header=True).load('data/DallasCouncilVoters.csv.gz')\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d71e45-20f6-4ed4-a272-3b06723f3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District|\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                             |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.select(voter_df['VOTER_NAME']).distinct().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902349c8-def0-4347-8865-03e3a8a04363",
   "metadata": {},
   "source": [
    "- VOTER NAME의 길이가 0보다 크고, 20보다 작은 값을 가져온다. \n",
    "- 그런데, 여전히 011018__42와 같은 숫자가 존재하는 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd29024-7773-48f6-bc4f-abb9e2ee0088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|011018__42         |\n",
      "|Mark  Clayton      |\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "voter_df.select('VOTER_NAME').distinct().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1926bac-d2cd-45c2-9b20-100c27114c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|Mark  Clayton      |\n",
      "|Casey Thomas       |\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 밑줄이 그어진 행은 제거하도록 한다. \n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "voter_df.select('VOTER_NAME').distinct().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63a855-cf04-4ed0-91c4-c2ee1a82cc8d",
   "metadata": {},
   "source": [
    "# Modifying DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42a6b09c-f1f9-416f-93aa-80ac21e7e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\green\\desktop\\pyspark_tutorial\\venv\\lib\\site-packages\\pyspark\\sql\\column.py:322: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95679809-9d9b-4a7e-b3c8-c51ff08143d3",
   "metadata": {},
   "source": [
    "# when() example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed65013-8ea4-408c-8272-ec4560df017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|          random_val|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.9510031922624299|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.11939166099331866|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.4688841647727736|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.8731323173833252|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.23039420617318918|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs| 0.38324748115748297|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|0.024511092826439373|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|  0.4834820010601497|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|  0.3908682374327499|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.6233786188803192|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.14962182905649002|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.3041223357174466|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.7270005830265949|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.7034285296117113|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|  0.7268094672395365|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.6604408962569257|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|  0.4415881507884907|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|  0.6459189418036481|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4d516d-a051-441c-8f41-0e75facfab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates| 0.2668185103788886|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.6710347119359316|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.7163574678648762|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.08872480661481152|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.6488034439724112|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs| 0.6015323813845656|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough| 0.8145376086226741|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|  0.914868366172406|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson| 0.5070083395597287|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|0.26234352179554377|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.5003663424136188|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.9768959780252076|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.27831935218192616|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.7919111734864849|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|0.24959279335948426|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.818885761808211|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson| 0.3990580089792608|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates| 0.8674995973237642|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6767d8ba-c854-4974-81d8-fee4ba05fc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|    Casey  Thomas|     Casey|   Thomas|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0ec17-ea69-43e3-a22f-8f4bb4034235",
   "metadata": {},
   "source": [
    "# Using user defined functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ecb1f69-98a9-4f8c-a936-00f4f059659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+-------------------+--------------------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|         random_val|              splits|first_and_middle_name|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+--------------------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates| 0.2668185103788886|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.6710347119359316|[Philip, T., King...|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|[Michael, S., Raw...|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.7163574678648762|     [Adam, Medrano]|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.08872480661481152|     [Casey, Thomas]|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.6488034439724112|[Carolyn, King, A...|         Carolyn King|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs| 0.6015323813845656|     [Scott, Griggs]|                Scott|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough| 0.8145376086226741| [B., Adam, McGough]|              B. Adam|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|  0.914868366172406|     [Lee, Kleinman]|                  Lee|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson| 0.5070083395597287|    [Sandy, Greyson]|                Sandy|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|0.26234352179554377|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.5003663424136188|[Philip, T., King...|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|[Michael, S., Raw...|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.9768959780252076|     [Adam, Medrano]|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.27831935218192616|     [Casey, Thomas]|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.7919111734864849|[Carolyn, King, A...|         Carolyn King|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|0.24959279335948426|[Rickey, D., Call...|            Rickey D.|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.818885761808211|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson| 0.3990580089792608|    [Sandy, Greyson]|                Sandy|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates| 0.8674995973237642|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95584fd8-aa22-4755-8f34-932e4813138a",
   "metadata": {},
   "source": [
    "# Adding an ID field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756a5744-f747-4acb-be44-8cf0cdae9688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').options(Header=True).load('data/DallasCouncilVoters.csv.gz')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb39bf6a-36df-4e38-94c8-ba5fa4b50dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n",
      "+--------------------+------+\n",
      "|          VOTER_NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|        Lee Kleinman|    35|\n",
      "|  the  final  201...|    34|\n",
      "|         Erik Wilson|    33|\n",
      "|  the  final   20...|    32|\n",
      "| Carolyn King Arnold|    31|\n",
      "| Rickey D.  Callahan|    30|\n",
      "|   the   final  2...|    29|\n",
      "|    Monica R. Alonzo|    28|\n",
      "|     Lee M. Kleinman|    27|\n",
      "|   Jennifer S. Gates|    26|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER_NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d5a6fa-875b-4c46-a52f-9481efa8fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|    35|\n",
      "|    36|\n",
      "|    37|\n",
      "|    38|\n",
      "|    39|\n",
      "|    40|\n",
      "|    41|\n",
      "|    42|\n",
      "|    43|\n",
      "|    44|\n",
      "|    45|\n",
      "|    46|\n",
      "|    47|\n",
      "|    48|\n",
      "|    49|\n",
      "|    50|\n",
      "|    51|\n",
      "|    52|\n",
      "|    53|\n",
      "|    54|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_2 = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df.select('ROW_ID').show()\n",
    "voter_df_2.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fd1df-8558-477d-9a5b-c8cd22986732",
   "metadata": {},
   "source": [
    "# Improving Performance\n",
    "- Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "010ffbe9-87b5-48f4-b6e3-a9f1b872d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 157198 rows took 3.112288 seconds\n",
      "Counting 157198 rows again took 1.266863 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Load the CSV file\n",
    "departures_df = spark.read.format('csv').options(Header=True).load('data/AA_DFW_2014_Departures_Short.csv.gz')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4cbb2-965e-4dbe-a848-b024ab1f7f58",
   "metadata": {},
   "source": [
    "- 처음 실행 시보다 두번째 작업할 때 속도가 좀 더 빨라진 것을 확인할 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948205d-3465-4a2b-87ff-d2b9384b7e12",
   "metadata": {},
   "source": [
    "## Removing DataFrame From Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96398114-c1b0-455a-a5b4-ce4b605d4d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447329f3-1edc-4a7a-815c-b69c53a6dad3",
   "metadata": {},
   "source": [
    "## File Import Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98771f7-1737-4b91-9062-2a4e09e97572",
   "metadata": {},
   "source": [
    "# Import the full and split files into DataFrames\n",
    "split_df = spark.read.csv('data/AA_*.csv.gz')\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474908b-a496-44a5-91ed-ea6c7df8fd5c",
   "metadata": {},
   "source": [
    "## Reading Spark Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c16440f1-5481-482c-a33e-61f3c141a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 200\n",
      "Name: dataCleansing\n",
      "Driver TCP port: 60975\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "print(\"Number of partitions: %s\" % num_partitions)\n",
    "# spark.conf.set('spark.sql.shuffle.partitions', 200)\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2127efc-3a61-4894-9d1f-2c5f5efdbc4b",
   "metadata": {},
   "source": [
    "## Normal Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f84a8f9-64eb-40cf-9b06-df3c1fe6b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|  name|price|\n",
      "+---+------+-----+\n",
      "|  1| apple| 1000|\n",
      "|  2|banana| 2000|\n",
      "|  3|tomato| 3000|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_01 = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['apple', 'banana', 'tomato'],\n",
    "    'price': [1000, 2000, 3000]\n",
    "})\n",
    "df_spark_01 = spark.createDataFrame(df_01)\n",
    "df_spark_01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8dd5ed2a-b8aa-4cbe-9ad9-8eca4273a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|quantity|\n",
      "+---+--------+\n",
      "|  1|       2|\n",
      "|  2|       4|\n",
      "|  4|       5|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_02 = pd.DataFrame({\n",
    "    'id': [1, 2, 4],\n",
    "    'quantity': [2, 4, 5]\n",
    "})\n",
    "df_spark_02 = spark.createDataFrame(df_02)\n",
    "df_spark_02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b2b8aed-81a2-48fa-b8e7-8d8678f04416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [id#1083L], [id#1115L], Inner\n",
      "   :- Sort [id#1083L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#1083L, 200), ENSURE_REQUIREMENTS, [id=#1472]\n",
      "   :     +- Filter isnotnull(id#1083L)\n",
      "   :        +- Scan ExistingRDD[id#1083L,name#1084,price#1085L]\n",
      "   +- Sort [id#1115L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#1115L, 200), ENSURE_REQUIREMENTS, [id=#1473]\n",
      "         +- Filter isnotnull(id#1115L)\n",
      "            +- Scan ExistingRDD[id#1115L,quantity#1116L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = df_spark_01.join(df_spark_02, \\\n",
    "    df_spark_01[\"id\"] == df_spark_02[\"id\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f62e1d-2c4c-484b-a361-6d95429b4529",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "- A couple tips:\n",
    "    + Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "    + On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "    + If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51ab2882-5f4b-4d93-8a63-68224b80ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [id#1083L], [id#1115L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(id#1083L)\n",
      "   :  +- Scan ExistingRDD[id#1083L,name#1084,price#1085L]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#1497]\n",
      "      +- Filter isnotnull(id#1115L)\n",
      "         +- Scan ExistingRDD[id#1115L,quantity#1116L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_df = df_spark_01.join(broadcast(df_spark_02), \\\n",
    "    df_spark_01[\"id\"] == df_spark_02[\"id\"] )\n",
    "\n",
    "# Show the query plan\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c253d-50e5-4978-9363-ad0e16098633",
   "metadata": {},
   "source": [
    "## Comparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e653f222-34f7-4553-a96a-4f4fc649e9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t2\tduration: 0.860768\n",
      "Broadcast count:\t2\tduration: 0.840761\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
